<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Transformer面经"><meta name="keywords" content="深度学习"><meta name="author" content="Mello13"><meta name="copyright" content="Mello13"><title>Transformer面经 | mello's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d5cba8acec7c962be4afb068976dd3fc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.2'
} </script><meta name="generator" content="Hexo 5.4.2"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%97%B6%E5%80%99loss%E4%BC%9A%E6%80%8E%E4%B9%88%E6%A0%B7"><span class="toc-number">1.</span> <span class="toc-text">过拟合的时候loss会怎么样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B7%E9%98%90%E8%BF%B0-transformer-%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%9D%A5%E8%A1%A8%E8%BE%BE%E5%92%8C%E7%94%9F%E6%88%90%E4%BF%A1%E6%81%AF%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%81%87%E8%AE%BE%E4%BB%80%E4%B9%88%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E6%88%96%E8%80%85%E5%85%AC%E5%BC%8F%E6%94%AF%E6%8C%81%E4%BA%86-transformer-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E8%AF%B7%E5%B1%95%E7%A4%BA%E8%87%B3%E5%B0%91%E4%B8%80%E4%B8%AA%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E5%85%B7%E4%BD%93%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">请阐述
Transformer
能够进行训练来表达和生成信息背后的数学假设，什么数学模型或者公式支持了
Transformer
模型的训练目标？请展示至少一个相关数学公式的具体推导过程。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">最大似然估计是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-transformer-%E8%83%BD%E5%A4%9F%E5%AF%B9-nlpcv-%E7%AD%89%E4%BB%BB%E4%BD%95-ai-%E9%A2%86%E5%9F%9F%E7%9A%84%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8C%E6%9C%89%E6%95%88%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.</span> <span class="toc-text">为什么
Transformer 能够对 NLP、CV 等任何 AI 领域的信息进行有效表示？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%B7%E6%8F%8F%E8%BF%B0-bert-%E4%B8%AD-mlm-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8Amlm%E5%AE%9E%E7%8E%B0%E4%B8%AD%E7%9A%84%E8%87%B3%E5%B0%91-5-%E4%B8%AA%E7%BC%BA%E9%99%B7%E5%8F%8A%E5%8F%AF%E8%83%BD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number"></span> <span class="toc-text">请描述
BERT 中 MLM 是什么以及MLM实现中的至少 5 个缺陷及可能的解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84-layer-normalization-%E8%95%B4%E5%90%AB%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8layer-norm-%E8%80%8C%E4%B8%8D%E6%98%AF-batch-normtransformer-%E6%98%AF%E5%90%A6%E6%9C%89%E5%85%B6%E5%AE%83%E6%9B%B4%E5%A5%BD%E7%9A%84-normalization-%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">Transformer
中的 Layer Normalization 蕴含的神经网络的假设是什么？为何使用Layer Norm
而不是 Batch Norm？Transformer 是否有其它更好的 Normalization
的实现</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Mello13</div><div class="author-info__description text-center">当未来没有来时，它还没有来</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">58</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xyzinc-xyx.github.io/">XYZinc</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">mello's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Transformer面经</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-03-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%88%B7%E7%BB%8F%E9%AA%8C/">刷经验</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="过拟合的时候loss会怎么样">过拟合的时候loss会怎么样</h3>
<p>在训练神经网络模型时，如果模型的复杂度过高或训练数据太少，就容易出现过拟合的情况，即模型在训练数据上表现得很好，但在未见过的新数据上表现不佳。</p>
<p>在过拟合的情况下，模型会在训练数据上得到非常低的损失值，但在验证集或测试集上的损失值会比较高。这是因为模型在训练数据上“记忆”了噪声或细节，导致其不能很好地泛化到未见过的数据。</p>
<p>因此，当模型出现过拟合时，训练损失会逐渐降低，但验证集或测试集上的损失会逐渐升高，直到出现明显的差异。这种情况下，需要采取措施来避免过拟合，例如增加训练数据、减少模型复杂度、使用正则化方法等。</p>
<p>解决过拟合的方法</p>
<p>以下是一些常见的解决过拟合问题的方法：</p>
<p>1.增加训练数据：增加训练数据是最简单的解决过拟合问题的方法之一。通过增加数据量，可以减少模型在训练数据上的“记忆”，从而使其更好地泛化到未见过的数据。</p>
<p>2.减少模型复杂度：过拟合通常是由于模型过于复杂而导致的，因此减少模型的复杂度是解决过拟合问题的有效方法之一。可以通过减少模型的层数、减小每层的神经元数量等方式来达到这个目的。</p>
<p>3.使用正则化方法：正则化是一种通过限制模型的参数来避免过拟合的方法。常见的正则化方法包括L1正则化、L2正则化和Dropout等。这些方法可以帮助模型更好地泛化到未见过的数据。</p>
<p>4.提前停止训练：提前停止训练是一种基于验证集损失的策略，当验证集损失不再下降时，就停止训练。这个方法可以避免模型在训练数据上过拟合，同时还可以提高训练效率。</p>
<p>5.数据增强：数据增强是一种通过对训练数据进行随机扰动来增加数据量的方法。这个方法可以使得模型更好地泛化到未见过的数据，并且可以提高模型的鲁棒性。</p>
<p>6.集成学习：集成学习是一种将多个不同的模型结合起来，从而得到更好的性能的方法。这个方法可以减少过拟合的风险，并且可以提高模型的泛化能力。</p>
<h3 id="请阐述-transformer-能够进行训练来表达和生成信息背后的数学假设什么数学模型或者公式支持了-transformer-模型的训练目标请展示至少一个相关数学公式的具体推导过程">请阐述
Transformer
能够进行训练来表达和生成信息背后的数学假设，什么数学模型或者公式支持了
Transformer
模型的训练目标？请展示至少一个相关数学公式的具体推导过程。</h3>
<p>Transformer
是一种基于自注意力机制的神经网络模型，用于处理序列到序列的任务，例如机器翻译、文本生成等。其能够进行训练来表达和生成信息，背后的数学假设是基于概率图模型和最大似然估计的思想。</p>
<p>具体来说，Transformer模型的训练目标是最大化给定输入序列 x 下目标序列
y 的条件概率：</p>
<p><span class="math inline">\(P(y|x)=\prod_{t=1}^{T}
P(y_t|y_{&lt;t},x)\)</span></p>
<p>其中，T 表示序列的长度，<span class="math inline">\(y_{&lt;t}\)</span> 表示目标序列 y 中的前 t-1
个标记。</p>
<p>为了方便计算，可以采用对数似然估计，将乘积转化为求和：</p>
<p><span class="math inline">\(\log P(y|x) = \sum_{t=1}^{T} \log
P(y_t|y_{&lt;t},x)\)</span></p>
<p>接着，Transformer
模型的训练目标可以表示为最小化负对数似然损失函数：</p>
<p><span class="math inline">\(L(\theta)=-\frac{1}{N} \sum_{i=1}^{N}
\sum_{t=1}^{T_i} \log P(y_{i,t}|y_{i,&lt;t},x_i;\theta)\)</span></p>
<p>其中，<span class="math inline">\(N\)</span>
表示训练样本的数量，<span class="math inline">\(\theta\)</span>
表示模型的参数。</p>
<p>为了实现这个目标，Transformer
模型使用了一个多层的编码器-解码器结构，并引入了自注意力机制和位置编码来处理序列信息。具体地，编码器通过自注意力机制来对输入序列进行编码，解码器通过注意力机制和自注意力机制来对编码器的输出进行解码和生成目标序列。</p>
<p>在训练过程中，Transformer
模型使用了反向传播算法和随机梯度下降优化器来最小化负对数似然损失函数，从而更新模型参数，使得模型能够更好地表达和生成信息。</p>
<p>总之，Transformer
模型的训练目标基于概率图模型和最大似然估计的思想，使用负对数似然损失函数来最小化条件概率，通过自注意力机制和位置编码来处理序列信息，使用反向传播算法和随机梯度下降优化器来更新模型参数。</p>
<h3 id="最大似然估计是什么">最大似然估计是什么</h3>
<p>最大似然估计是一种常用的参数估计方法，用于从数据样本中推断出最合适的参数值。它的基本思想是，给定一组观测数据，假设这些数据是由一个概率分布生成的，然后通过寻找最大化观测数据的概率的参数值来估计概率分布的参数。</p>
<p>具体来说，假设观测数据 <span class="math inline">\(x_1, x_2, ...,
x_n\)</span> 是独立同分布的样本，其概率密度函数为 <span class="math inline">\(p(x|\theta)\)</span>，其中 <span class="math inline">\(\theta\)</span>
是待估计的参数。那么，给定观测数据后，最大似然估计就是通过寻找最大化样本的联合概率密度函数
<span class="math inline">\(p(x|\theta)\)</span> 的参数 <span class="math inline">\(\theta\)</span> 来估计模型参数，即：</p>
<p><span class="math inline">\(\hat{\theta}{ML} =
\operatorname*{argmax}\theta \prod_{i=1}^{n} p(x_i|\theta)\)</span></p>
<p>为了方便计算，通常使用对数似然函数来表示最大化目标：</p>
<p><span class="math inline">\(\hat{\theta}{ML} =
\operatorname*{argmax}\theta \sum_{i=1}^{n} \log
p(x_i|\theta)\)</span></p>
<p>最大似然估计是一种常用的参数估计方法，它具有估计结果唯一、无偏性、渐进有效性等优点，被广泛应用于各种领域，例如统计学、机器学习、信号处理、自然语言处理等。</p>
<h3 id="为什么-transformer-能够对-nlpcv-等任何-ai-领域的信息进行有效表示">为什么
Transformer 能够对 NLP、CV 等任何 AI 领域的信息进行有效表示？</h3>
<p>Transformer
是一种基于自注意力机制的神经网络模型，其主要特点是可以并行计算和捕捉长距离依赖关系。这些特性使得
Transformer
在自然语言处理和计算机视觉等各种人工智能领域中表现出色，并成为了目前最为流行的神经网络模型之一。</p>
<p>具体来说，Transformer 在 NLP 领域的应用中，主要有以下几个优势：</p>
<p>自注意力机制：Transformer
中引入了自注意力机制，可以对输入序列中的每个位置进行加权汇聚，从而捕捉不同位置之间的依赖关系，能够更好地处理长文本输入，避免了
RNN 等传统模型中存在的梯度消失和梯度爆炸问题。</p>
<p>并行计算：Transformer 的计算是完全并行的，可以利用 GPU
等硬件加速，在处理大规模数据时具有较高的计算效率。</p>
<p>多层表示学习：Transformer
可以通过堆叠多个编码器和解码器层，学习到更丰富的语义表示，并在这些层之间进行信息流动和交换，从而提高模型的表达能力和泛化性能。</p>
<p>对于 CV 领域的应用，Transformer 主要有以下优势：</p>
<p>可以对任意形状的输入进行编码和解码：传统的 CNN
模型仅适用于固定大小的输入图像，而 Transformer
可以适应任意大小的输入，因此更适合于处理不同尺寸和形状的图像输入。</p>
<p>可以学习到全局特征：由于自注意力机制的存在，Transformer
可以对输入图像中的所有位置进行加权汇聚，从而学习到全局的特征表示。这种全局特征可以提高模型的分类准确率和泛化性能。</p>
<p>可以处理多模态输入：对于一些包含多种不同类型信息的数据集，如图像标注数据集，Transformer
可以方便地处理多模态输入，同时学习到不同模态之间的关联性。</p>
<p>因此，由于其自注意力机制和并行计算等特性，Transformer
在处理各种类型的输入数据时具有良好的表达和泛化能力，可以被广泛应用于自然语言处理、计算机视觉和其他人工智能领域。</p>
<h2 id="请描述-bert-中-mlm-是什么以及mlm实现中的至少-5-个缺陷及可能的解决方案">请描述
BERT 中 MLM 是什么以及MLM实现中的至少 5 个缺陷及可能的解决方案</h2>
<p>BERT中的MLM（Masked Language
Model）是指在训练时，将输入序列中的一部分token随机地被替换为一个特殊的MASK标记，模型的目标是预测被MASK标记替换的token。通过这种方式，模型可以在训练时自学习语言知识，同时还可以避免预测目标词与上下文词之间存在关联的问题。</p>
<p>然而，MLM也存在一些缺陷，如下所述：</p>
<p>数据稀疏性问题：由于BERT采用随机MASK一部分token的方式，可能导致某些token被MASK掉的次数较少，从而导致该token的表示向量学习不到充分的语义信息，进而影响模型的表现。
解决方案：可以采用更加均衡的MASK策略，例如固定MASK一定比例的token，或者根据词频信息进行MASK等。</p>
<p>掩盖区域长度问题：MASK操作可能导致一些原始输入中的短语被完全MASK掉，进而影响模型对该短语的语义理解。
解决方案：可以采用连续的MASK策略，即对一段连续的token进行MASK，或者采用部分MASK策略，即仅MASK短语的一部分。</p>
<p>无法预测未见过的词汇问题：由于MASK操作会生成一些模型未见过的词汇，从而无法在训练中进行学习，可能导致模型预测效果不佳。
解决方案：可以采用一些词汇预测技术，例如使用外部的语言模型对未见过的词汇进行预测。</p>
<p>预测目标太少问题：在MLM中，被MASK的token数量较少，因此模型需要从更少的目标中学习，可能导致模型性能下降。
解决方案：可以采用双向的MLM策略，即对输入序列中的部分token进行MASK和预测，同时对原始序列进行逆序处理，再次进行MASK和预测，从而增加预测目标。</p>
<p>无法处理不确定性问题：在训练时，BERT中的MLM仅能预测被MASK的token，无法处理不确定性问题，例如在输入中存在多个可能的短语时，无法确定应该MASK哪一个短语。
解决方案：可以采用更加灵活的MASK策略，例如根据短语边界信息进行MASK，或者使用可控的MASK策略，例如对于输入中存在多个可能短语的情况，可以采</p>
<h3 id="transformer-中的-layer-normalization-蕴含的神经网络的假设是什么为何使用layer-norm-而不是-batch-normtransformer-是否有其它更好的-normalization-的实现">Transformer
中的 Layer Normalization 蕴含的神经网络的假设是什么？为何使用Layer Norm
而不是 Batch Norm？Transformer 是否有其它更好的 Normalization
的实现</h3>
<p>Layer Normalization和Batch
Normalization都是用来加速神经网络训练过程的方法，它们的区别在于归一化的层级不同。Batch
Normalization是对每个batch中的数据进行归一化，而Layer
Normalization是对每个样本（或每个样本的特征）进行归一化。</p>
<p>Layer
Normalization蕴含的神经网络的假设是每个神经元的输出是相互独立的，而Batch
Normalization蕴含的假设是每个神经元的输出与整个batch的分布相关。在自然语言处理等应用中，由于每个样本的长度和结构不同，使用Layer
Normalization相对更加合适。</p>
<p>另外，Batch Normalization在每个batch内进行归一化，可能会导致batch
size过小的问题，从而影响归一化的效果。相比之下，Layer
Normalization则没有这个问题，因为它是在每个样本（或每个样本的特征）上进行归一化。</p>
<p>虽然在Transformer中采用了Layer
Normalization，但还存在一些其他的Normalization方法，例如Instance
Normalization、Group
Normalization等。这些方法都是为了在不同的应用场景下更好地加速神经网络的训练过程。在实际应用中，应根据具体情况选择最合适的Normalization方法。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Mello13</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://skylersuen.github.io/2023/03/20/Transformer面经/">https://skylersuen.github.io/2023/03/20/Transformer面经/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/03/27/%E6%B7%98%E5%A4%A9%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95%E5%9B%9E%E5%BF%86/"><i class="fa fa-chevron-left">  </i><span>24暑期实习淘天笔试</span></a></div><div class="next-post pull-right"><a href="/2023/03/09/Day41/"><span>刷题Day41——动态规划:01背包</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2022 - 2024 By Mello13</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">if you have any suggestion, please contact me at syf_mello@163.com !</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>